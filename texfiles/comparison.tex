\section{Comparison of models}
To get an overview in terms of LUT-based models, AIGs and other common ML algorithms, we evaluate training and testing accuracies and model size, i.e. we create the data from which Figure~\ref{fig:model_performance} is generated. The convolutional neural network has the highest accuracy, but is bigger and requires floating-point matrix operations. Given the right choice of architecture and algorithm, LUT-based models are performing quite well, considering reduced size and possible bit-level implementation. The best AIG that we found lacks good accuracy compared to most other models, but is nevertheless very promising with its ultra-small file size of just 68 Bytes.

\subsection{Accuracy and size} \label{sec:models}
As in \cite{bib:chatterjee2018learning}, we train various machine learning algorithms on the Binary-MNIST dataset and compare their performance. The training and testing accuracies can be seen in Table~\ref{tab:ml_algos_on_bmnist} and testing accuracies are visualized in Figure~\ref{fig:model_performance}. Each LUT network has five hidden layers and 1024 LUTs per hidden layer, except the \enquote{8-LUT network slim} which has five hidden layers and 256 LUTs per hidden layer. For the reader wishing to refresh their knowledge about convolutional neural networks, \cite{bib:yamashita2018convolutional} provides a good overview. We implemented the CNN using the Python package PyTorch \cite{bib:NEURIPS2019-9015}. The CNN has two convolutional layers with 64 and 32 filters and 2 fully connected layers with 256 and 128 neurons. After the convolutions, we perform max pooling and we use ReLU as activation function between the layers. The output has a sigmoid activation function. We use the Adam optimizer and go through the dataset only once, i.e., train for one epoch. For all other learning algorithms, we use the Python package scikit-learn \cite{bib:scikit-learn}. Except the parameters that we explicitely state, we use the default parameters from scikit-learn. We can see that a 12-LUT network beats Logistic Regression and Naïve Bayes. The best performing model is a convolutional neural network. Our results are the same as in \cite{bib:chatterjee2018learning}, take or leave 1\%.

\noindent Another interesting aspect to consider is how much disk space these models take up. In Table~\ref{tab:ml_algos_on_bmnist} the last column shows the size of each model in Kilobytes, the derivation of which can be seen in Appendix~\ref{app:size}. We should note that the sizes presented here are only an approximation and the actual implementation on hardware will always be bigger. In terms of accuracy, the convolutional neural network performs best. It is fairly big, though, and could be made smaller by a technique called \enquote{pruning} \cite{bib:molchanov2016pruning}. An advanced ensemble of 1024 2-LUTs with 5 layers and 64 LUTs per layer has a nice trade-off between performance and size. The AIG is one of the models with the worst accuracy, however the accuracy is still significantly above 0.50 meaning learning had taken place. With just 68 Bytes, it is by far the smallest learned model.

\begin{table}[!htb]
    \begin{small}
        \begin{tabular}{llll}
        Algorithm & Training    & Testing    & Size               \\
                  & accuracy    & accuracy   &                    \\ \hline
        Convolutional Neural Network & 0.99  & 0.99 & 1164 KB     \\
        5-Nearest Neighbors          & 0.99  & 0.97 & 5881 KB     \\
        1-Nearest Neighbors          & 1.00  & 0.97 & 5881 KB     \\
        Random Forests (10 trees)    & 1.00  & 0.96 & 3797 KB     \\
        (5@64)$\times$1024 2-LUT advanced ensemble    & 0.94  & 0.94 & 168 KB      \\
        5@1024 12-LUT network        & 0.99  & 0.90 & 2622 KB     \\
        5@1024 10-LUT network        & 0.94  & 0.88 & 655 KB      \\
        5@1024 8-LUT network         & 0.89  & 0.87 & 164 KB      \\
        5@256 8-LUT network          & 0.88  & 0.87 & 41 KB       \\
        Logistic Regression          & 0.87  & 0.87 & 3 KB        \\
        (5@64)$\times$1024 2-LUT plain ensemble     & 0.78  & 0.79 & 168 KB      \\
        Naïve Bayes                  & 0.77  & 0.77 & 16 KB       \\
        AIG (193 AND-gates)          & 0.76  & 0.76 & 0.0068 KB   \\
        Random Guess                 & 0.50  & 0.50 & 0.0004 KB
        \end{tabular}
    \end{small}
    \caption{Performance of various ML algorithms on the Binary-MNIST dataset. How the sizes of the parameters were computed can be seen in Appendix~\ref{app:size}. }
\label{tab:ml_algos_on_bmnist}
\end{table}

\noindent Table~\ref{tab:ml_algos_on_bmnist} does not fully do justice to the binary models, i.e., LUT networks and AIGs because binary models have to potential to be implemented without floating-point computation and are thus much faster. An exception would be the advanced LUT ensembling from Section~\ref{sec:ada_boost} which utulizes parameters that are floats. However, apart from these parameters which are not many to begin with (as much as there are individual LUT networks), everything is in binary and thus can be implemented fast. AIGs are excellent in terms of size and speed; they are extremely small and can be easily implemented natively on hardware. However, finding a good AIG is hard, i.e., there is need for a much better search algorithm.

\subsection{Summary}
We evaluated training and testing accuracies for LUT-based models and AIGs, along with other popular machine learning models. The best performing model is the convolutional neural network, however, it is fairly big and requires floating-point computation. The advanced lut ensemble has an accuracy that is fairly good, along with a much smaller model size compared to the CNN. We saw that the choice of architecture, especially the bit-size, for LUT-based models is crucial; a bad choice will result in a big size with questionable or no improvement in accuracy. The AIG has a relatively low accuracy compared to all other models, however, it is extremely small and thus very suitable for implementation on small electronic devices with little compute. It is important to make ourselves clear that Table~\ref{tab:ml_algos_on_bmnist} does not discuss inference speed, LUT-based models and AIGs work in binary and have the potential to work much faster than other machine learning models.
