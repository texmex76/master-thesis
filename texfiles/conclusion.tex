\section{Conclusion}
In this thesis, we took two existing circuit designs and investigated the possibility and practicality of using them as binary predictive systems, mostly by experimental approaches, where we used a binaried version of MNIST as training and testing data. The first circuit design was a network of LUTs. We started with the definition of a single LUT and then we considered LUT networks that are made of many individual LUTs. We recreated one experiment from the paper that introduced LUTs. Then we took the idea of LUT networks further and devised different schemes to improve the accuracy, where we took three main approaches, i.e., modifying either existing LUT networks or the LUT learning algorithm, enhancing the dataset using feature-engineering and constructing ensembles of many LUT networks. Using LUT ensembles, we could improve the accuracy, together with obtaining a reduced model size, if the architecture is chosen appropriately. The second circuit design was an AIG. We devised an initialization scheme from scratch and performed a local greedy search to find a suitable architecture. The search algorithm worked, albeit the resulting AIGs had a much lower accuracy than the LUT-based models. However, given the tiny model size of the AIGs, they are very promising to be implemented for small electronic devices or even implementation on hardware. To get an overview, we summarized model accuracies and size for LUT-based models, AIGs and other ML algorithms in a Table.


\subsection{Findings and limitations}
To run experiments with LUT-based models, we wrote our own code in Python, where the most important external package was NumPy. The code is available on GitHub~\cite{bib:lut_github}. We recreated one experiment from the original LUT paper \cite{bib:chatterjee2018learning} and our results were the same. We found that the number of LUTs per hidden layer can sometimes be reduced, resulting in a smaller size with almost no performance drop. A majority vote and maximizing the layer-wise mean accuracy while training help improving the performance of single LUT networks of low bit-size. For higher bit-sizes, however, the change does not surpass 1-2\%. Feature engineering gave small improvements in accuracy. Ensembling techniques are promising, especially an AdaBoost.M1 ensemble achieved a testing accuracy higher than any previous single LUT networks. With a good choice of architecture, we were able to reduce the size of the ensemble significantly while retaining accuracy. We also obtained results that suggest combining ensembling and maximizing the layer-wise mean accuracy is not a good idea.

We wrote the code for our AIG experiments in C++ from scratch, making use of the AIGER library. The code is available on GitHub~\cite{bib:aig_github}. We found that the AIG local search algorithm works and we were able to iteratively improve the accuracy. Because of computational time constraints, we restricted the search space to architectures with a maximum of 192 AND-gates. The best local search run returned an AIG that has an accuracy significantly above chance.

Evaluating training and testing accuracies for LUT-based models, AIGs and other machine learning models, the convolutional neural network had the highest accuracy out of all models. A CNN, however, is bigger and requires floating-point matrix operations. Given the right choice of architecture and algorithm, LUT-based models performed quite well, considering reduced size and possibility of bit-level implementation. The choice of architecture (especially the bit-size) for LUT-based models is crucial; a bad choice will result in a big size with questionable improvement in accuracy. The best AIG that we found lacked good accuracy compared to most other models, but is nevertheless very promising with its very small file size, thus possibly suitable for implementation on small electronic devices with little computing power. 

\subsection{Future work}
Further improving accuracy and reducing size of LUT-based models is key for making them viable for popular use. We have not discussed which percentage of a LUT network is inactive and can thus be pruned. As of now, it is unclear how to initialization of AIGs influenes the search process. Better search algorithms have to investigated to find AIGs faster and with higher accuracy. We have also not discussed multi-class classification which would be possible with both LUT-based models and AIGs using an one-vs-one or one-vs-all approach similar to SVMs \cite{bib:bishop2006pattern}.
