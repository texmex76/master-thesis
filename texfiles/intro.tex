\section{Introduction}

\subsection{Background}
Neural networks trained using gradient descent work remarkably well which is the reason for their universal use. Small neural networks have their place, but it is the large neural networks that perform complex tasks, such as translation, object recognition or autonomous driving. The more parameters, the more compute during training and inference is required and the environmental impact is a concern \cite{bib:schwartz2020green}. A cumbersome aspect of neural network-powered AI is that many models are stored on a remote server and data has to be sent over the internet. The requirement of a constant and (in most cases) fast internet connection is a serious inhibitor to applying neural networks in practice. Without being able to transmit all of its input data to a server, devices with small computing power and little storage, such as autonomous lawn mowers, intelligent cameras, smart sensors, etc, have no means to run a big neural netork.

\subsection{Approach}
We know that a neural network basically is just a sequence of matrix operations; our input is transformed and we obtain a quantity of our interest which could be a probability, a number, a synthesized image, etc. We also know that a computer works with binary values, i.e., 0s and 1s. In the end, a representation of a neural network is a sequence of 0s and 1s, so is the input to the neural network and the output, which raises the question if we can \textit{directly} work with binary values, skipping layers of abstraction. Perhaps such a scheme would be more environmentally friendly since it does not require floating point operations. A small binary model that requires little compute and disk space might be a good step towards making AI more prevalent and reducing costs. Extensive research already exists to make existing neural architectures smaller to apply them at edge devices \cite{bib:liu2021bringing} which also includes binary architectures. However, most existing approaches to reduce model size into binary start with regular models and gradually transfer them to binary. We already start at binary.

\subsection{Outline}
Our questions of interest are summarized in the list below. We address (\ref{enum:build}) by looking at 2 schemes to build network-like architectures, where the former is described in a paper \cite{bib:chatterjee2018learning} and the latter we invent ourselves. We evaluate training and testing accuracies of each architecture (\ref{enum:performance}), calculate the size they take up on disk (\ref{enum:disk_space}) and discuss the computational power required to deploy these models (\ref{enum:efficiency}). The sub-items of (\ref{enum:practical}) we discuss to our best knowledge, but might only give a hint what is possible given there is room for improvement (\ref{enum:room}).

\begin{enumerate}
  \item \label{enum:build} Is it possible to build a predictive system that works entirely in binary?
  \item \label{enum:practical} Is it practical to build a predictive system working in binary? Concretely, does a predictive system working with binary values...
    \begin{enumerate}
      \item \label{enum:performance} ...have a good performance in terms of accuracy?
      \item \label{enum:disk_space} ...take up less space on disk?
      \item \label{enum:efficiency} ...have a good performance in terms of computational efficiency?
    \end{enumerate}
  \item \label{enum:room} Is there room for improvement?
\end{enumerate}

\noindent Lastly, we would like to already give away what we achieved in this thesis. Figure~\ref{fig:model_performance} visualizes testing accuracies and size for different models. The more a point lies to the top and to the left, the better. The meaning of the model names we will introduce throughout this thesis. We took an existing approach of binary model from \cite{bib:chatterjee2018learning}, visible as $\color{green} \oplus$ in Figure~\ref{fig:model_performance}, and were able to improve accuracy and reduce size, visible as $\color{violet} \bigtriangleup$ in Figure~\ref{fig:model_performance}. On the lower-left corner we can see another binary model that we built, the \enquote{AIG}. It is the weakest model in terms of accuracy in Figure~\ref{fig:model_performance}, however, it is extremely small (just 68 Bytes) and easily translatable to a circuit on hardware, making this model a very good candidate for implementing on small electronic devices, also called \enquote{edge devices}.

\begin{figure}[!htb]
    \centering
    \includestandalone[]{standalone/lut/model_performance}
    \caption{Testing accuracy on the dataset \enquote{Binary-MNIST} and size for various models. The $x$-axis that represents the size has a logarithmic scale. The underlying data that was used to generate this plot can be seen in Table~\ref{tab:ml_algos_on_bmnist}. Our main contributions are $\color{violet} \bigtriangleup$, a binary model with improved accuracy and reduced sized compared to an existing approach $\color{green} \oplus$ and $\otimes$, a model that is extremely small and fast, albeit with lower accuracy.}
\label{fig:model_performance}
\end{figure}
\FloatBarrier
