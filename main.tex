\documentclass[a4paper,12pt]{article}


\input{texfiles/packages.tex}

\begin{document}
\usetikzlibrary{patterns}

%
    \title{Tuning learning of circuit-based classifiers\\
    \vspace{2em}
    Master's Thesis\\
    \vspace{2em}
    JKU Linz\\
    \vspace{1.5em}
    from}

    \author{
 	\LARGE Bernhard Gstrein\\
 	\vspace{.5em} \\
 	Supervisors:\\
 	Martina Seidl\\
 	Armin Biere\\
 	\vspace{1em}
	}
    
\date{Linz, 2022}

\maketitle
   
\begin{abstract}
\noindent Neural networks have proven to work remarkably well and are prevalent in our everyday lives. Two major concerns, however, are computational cost while training and during inference and the model size. To address these concerns, we remind ourselves that on a computer, everything is represented by 0s and 1s. Working at low level could be a step towards efficiency in terms of compute and disk space. This thesis explores the questions whether building a predictive system that natively works with binary values is possible and in the case that this is true, whether there are any advantages compared to gradient descent with backpropagation at its core.
\end{abstract}
   
\newpage
   
\tableofcontents
 
\newpage
    

\input{texfiles/intro.tex}
\input{texfiles/preliminaries.tex}
\input{texfiles/lut.tex}
\input{texfiles/improve_lut.tex}
\input{texfiles/aig.tex}
\input{texfiles/comparison.tex}
\input{texfiles/conclusion.tex}

\bibliographystyle{plain}
\bibliography{bibliography}

\input{texfiles/appendix.tex}

\end{document}
