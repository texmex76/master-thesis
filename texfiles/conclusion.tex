\section{Conclusion}
We studied whether it is possible to build a classifier working with binary values directly at binary level. Concretely, we tried out 2 different schemes, LUT networks and AIGs, where we found that LUT networks can be built in reasonable time, have reasonable performance and given the right architecture choice take up little space on disk. AIGs also have to potential to work as binary classifiers. We introduced what AIGs are and presented an initialization and learning scheme.

\subsection{Findings and Limitations}
LUT networks could be a good alternative to traditional machine learning models in some cases. In Section~\ref{sec:models} we have seen inferior performance of LUT networks on Binary-MNIST compared to a CNN. However, we were able to improve performance significantly in Section~\ref{sec:ada_boost}, not to the extent to the CNN, but perhaps with more research, the performance of LUT-based models can be increased even further. When it comes to model size, the choice of architecture matters a lot in LUT networks. Especially the bit-size is crucial; since the size of a LUT table is exponentially proportional to the bit size, a too large bit size quickly increases size on disk with questionable improvement in performance, see Section~\ref{sec:models}. As we have seen in Section~\ref{sec:num_luts_per_layer}, we can significantly decrease the nubmer of LUTs per hidden layer in a LUT network without much loss of accuracy. Making a good choice of architecture, combined with ensembling techniques results in LUT-based models that are comparitively small. One other thing we should note is that LUT-based models lack the need for matrix operations as opposed to neural networks which could result in increased inference speed.

The use of AIGs as classifiers that are trained like a regular machine learning model seems possible. We, however, found much lower performance than LUT networks which we think is due to 3 main reasons: possibly sub-optimal initialization, a too na√Øve learning algorithm and an implementation that lacks optimization, especially multi-processing. Perhaps given more research, performance can be significantly increased. Since we devised initialization, learning and the implementation from scratch, we think that there is much room for improvement.

With Binary-MNIST dataset we used, we obtained training and testing accuracies of 0.94 for ensembles of 2-LUT newtorks. Starting directly at low-level representation instead of at higher level and then going down is thus possible.

\subsection{Recommendations}
The task treated in this thesis is to predict something. Given a training set, we utulize a learning algorithm and in the end we would like to obtain a model that has a good performance in terms of accuracy. So there are ambiguities; we do not know if the dataset is entirely correct and if the prediction is exactly what we want. Logic synthesis \cite{bib:logic_synthesis_wiki} is a field where specifications and constraints are given and an algorithm produces a structure/circuit. These systems are understood as doing a specific task as opposed to predicting something. In the regime of prediction, if starting out with a neural network, there already exist model size reducing schemes \cite{bib:liu2021bringing} and quite a bit of research is being done in this area. The model developer needs to consider which approach is best suited for development, whether it be starting at high level and then going down or already starting at low level as in this thesis. LUT ensembling presented in Section~\ref{sec:ada_boost}, together with a clever choice of a small architecture, could already be useful for a production environment depending on the task.

\subsection{Future Work}
Further increasing the accuracy of LUT-based models is key for making them viable for popular use. Section~\ref{sec:ada_boost} gives a hint that LUT-based models could give a powerful performance. A proper LUT framework that runs fast (e.g., C++ instead of Python), is easy to use and also suitable for production environment could help introducing the idea of LUTs to a wider audience. What we have not investigated is if LUTs could be used as an intermediate representation of circuits. Also, if LUTs can be implemented easily on hardware. When it comes to AIGs as classifiers, a proper initialization and learning algorithm have yet to be devised. AIGs as classifiers definitely have potential, but we are yet unsure how much.
