\appendix

\section{Appendix}

\subsection{Derivation of parameter sizes in Section~\ref{sec:models}} \label{app:size}
The actual size on disk of a machine learning model heavily depends on its implementation which is the reason we specify only the \textit{size of all parameters} in Table~\ref{tab:ml_algos_on_bmnist} and not the actual model size. Nevertheless, the size of all parameters still gives us an insight since the machine instructions what to do with the parameters do not make up as much space. Also, some machine learning models do not have any parameters that are comparable to neural network weights or LUT table entries, such as Nearest Neighbors. For those models, the entry in Table~\ref{tab:ml_algos_on_bmnist} stays empty.

The size of the LUT networks is computed using Equation\ref{eq:lut_size}.

For a convolutional neural network, we consider the total number of all trainable parameters (i.e., all weight entries) which we obtain from the PyTorch model in one line of code. We assume 32-bit floats.

We determine the size of all parameters of Logistic Regression and Na√Øve Bayes by going through all methods the respective scikit-learn object has and considering its size if it is either an int, float or Numpy array.

Since Random Forests are made up of multiple decision trees, we go through each of them. In scikit-learn a decision tree has the method \hl{\texttt{tree\_}} and we consider what it contains for the size.
