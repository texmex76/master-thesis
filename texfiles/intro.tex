\section{Introduction}
In order to learn the parameters of a neural network, backpropagation is almost exclusively used nowadays. Of course backpropagation is not the whole story, it merely provides the gradients of the weights with respect to some loss function. Since the loss function has to be minimized, the gradients of it with respect to the neural network weights provide us a direction where we want to go. We thus update the weights by subtracting the current gradients, scaled by a parameter called \enquote{learning rate}, from it. This procedure is called \enquote{gradient descent} and can be used with many different loss functions for many different problems which includes models that are not neural networks. Nowadays gradient descent is refined by adding many heuristics to it, for example choosing the optimal step per parameter, the concept of \enquote{momentum}, batching and many more.

Gradient descent with backpropagation at its core works remarkably well which is the reason for its universal use in machine learning. However, it is computationally expensive. We know that a neural network basically is just a succession of matrix operations; our input is being transformed and in the end we obtain a quantity of our interest which could be a probability, a number, a synthesized image, etc.. We also know that a computer works with binary values (i.e. 0s and 1s). In the end, a representation of a neural network is a successions of 0s and 1s, so is the input to the neural network and the output. That raises the question if we can directly work with binary values, skipping layers of abstraction. This thesis explores the questions whether this is possible and in the case that this is true, whether there are any advantages compared to gradient descent with backprop at its core.
